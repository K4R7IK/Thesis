This chapter presents a comprehensive methodology for adaptive detection of fileless and polymorphic malware through an integrated approach combining memory forensics, computer vision, and machine learning techniques. The proposed framework addresses the fundamental limitations of traditional signature-based detection systems by analyzing memory-resident patterns and employing advanced dimensional reduction techniques to enhance unknown malware identification capabilities.

\section{Overview of the Proposed Framework}
\label{sec:methodology-overview}

The methodology encompasses a multi-phase approach designed to capture, analyze, and classify malware behavior through memory-based analysis. Figure~\ref{fig:methodology-overview} illustrates the complete workflow of our proposed detection framework.

\begin{figure}[!htbp]
\centering
    \begin{tikzpicture}
        % NOTE: Your main .tex file MUST have \usepackage{tikz} and \usetikzlibrary{positioning} in the preamble.
        \node[draw, rectangle, text width=3cm, text centered] (collect) {Memory Dump Collection};
        \node[draw, rectangle, text width=3cm, text centered, below=1cm of collect] (convert) {RGB Image Conversion};
        \node[draw, rectangle, text width=3cm, text centered, below=1cm of convert] (extract) {Feature Extraction};
        \node[draw, rectangle, text width=3cm, text centered, below=1cm of extract] (classify) {Classification};
        \node[draw, rectangle, text width=3cm, text centered, below=1cm of classify] (detect) {Unknown Malware Detection};
        
        \draw[->] (collect) -- (convert);
        \draw[->] (convert) -- (extract);
        \draw[->] (extract) -- (classify);
        \draw[->] (classify) -- (detect);
    \end{tikzpicture}
    \caption{Overall methodology workflow for malware detection framework}
    \label{fig:methodology-overview}
\end{figure}

\section{Malware Characteristics and Detection Challenges}
\label{sec:malware-characteristics}

\subsection{Polymorphic Malware Analysis}
\label{subsec:polymorphic-analysis}

Polymorphic malware represents a sophisticated class of threats that employ multiple obfuscation techniques to evade detection systems. These techniques fundamentally alter the binary representation while preserving the underlying malicious functionality.

\subsubsection{Code Transformation Techniques}

The polymorphic transformation process involves several sophisticated mechanisms:

\begin{enumerate}
    \item \textbf{Encryption-based Polymorphism}: The malware encrypts its payload using varying encryption keys for each infection instance. The decryption routine remains constant, but the encrypted body appears completely different across infections.
    
    \item \textbf{Instruction Substitution}: Functionally equivalent instruction sequences replace original code segments. For example, the instruction \texttt{MOV EAX, 0} might be replaced with \texttt{XOR EAX, EAX}.
    
    \item \textbf{Register Reassignment}: The malware randomly assigns different CPU registers for equivalent operations, altering the binary signature without affecting functionality.
    
    \item \textbf{Dead Code Insertion}: Non-functional instruction sequences are randomly inserted between operational code segments to modify the overall signature.
    
    \item \textbf{Code Transposition}: Instruction blocks are reordered while maintaining logical flow through jump instructions and conditional branches.
\end{enumerate}

Algorithm~\ref{alg:polymorphic-analysis} presents our approach to analyzing polymorphic characteristics in memory dumps.

\begin{algorithm}[!htbp]
\caption{Polymorphic Pattern Analysis in Memory Dumps}
\label{alg:polymorphic-analysis}
\begin{algorithmic}[1]
\Require Memory dump $M$, baseline signatures $S_{base}$
\Ensure Polymorphic transformation indicators $P_{indicators}$
\State $P_{indicators} \leftarrow \emptyset$
\State $blocks \leftarrow$ ExtractCodeBlocks($M$)
\For{each $block \in blocks$}
    \State $patterns \leftarrow$ AnalyzeInstructionPatterns($block$)
    \State $entropy \leftarrow$ CalculateEntropy($block$)
    \If{$entropy > threshold_{high}$}
        \State $P_{indicators} \leftarrow P_{indicators} \cup \{$"encryption detected"$\}$
    \EndIf
    \State $substitutions \leftarrow$ DetectInstructionSubstitution($patterns$, $S_{base}$)
    \If{$|substitutions| > threshold_{substitution}$}
        \State $P_{indicators} \leftarrow P_{indicators} \cup \{$"instruction substitution detected"$\}$
    \EndIf
    \State $dead\_code \leftarrow$ IdentifyDeadCode($block$)
    \If{$|dead\_code| > threshold_{dead}$}
        \State $P_{indicators} \leftarrow P_{indicators} \cup \{$"dead code insertion detected"$\}$
    \EndIf
\EndFor
\Return $P_{indicators}$
\end{algorithmic}
\end{algorithm}

\subsection{Fileless Malware Characteristics}
\label{subsec:fileless-characteristics}

Fileless malware operates exclusively within system memory, presenting unique detection challenges due to its volatile nature and minimal forensic footprint.

\subsubsection{Memory-Only Execution Patterns}

Fileless malware exhibits several distinctive memory-based execution patterns:

\begin{itemize}
    \item \textbf{Process Injection}: Malicious code injection into legitimate running processes using techniques such as DLL injection, process hollowing, or reflective loading.
    
    \item \textbf{Living-off-the-Land}: Exploitation of legitimate system utilities (PowerShell, WMI, CMD) to execute malicious operations without deploying custom executables.
    
    \item \textbf{Registry-based Persistence}: Storage of encrypted payloads within Windows Registry keys or other non-traditional persistence mechanisms.
    
    \item \textbf{Script-based Execution}: Direct execution of malicious scripts (JavaScript, VBScript, PowerShell) through legitimate interpreters.
\end{itemize}

Algorithm~\ref{alg:process-injection} demonstrates our approach to detecting process injection patterns:

\begin{algorithm}[!htbp]
\caption{Process Injection Detection Algorithm}
\label{alg:process-injection}
\begin{algorithmic}[1]
\Procedure{DetectInjectionPatterns}{$MemoryDump$}
    \State $injection\_indicators \leftarrow \emptyset$
    \State $processes \leftarrow$ EnumerateProcesses($MemoryDump$)
    \For{each $process \in processes$}
        \If{IsProcessHollow($process$)}
            \State Add "process\_hollowing" indicator to $injection\_indicators$
        \EndIf
        \State $memory\_regions \leftarrow$ GetMemoryRegions($process$)
        \For{each $region \in memory\_regions$}
            \If{IsInjectedCode($region$)}
                \State Add "code\_injection" indicator to $injection\_indicators$
            \EndIf
        \EndFor
    \EndFor
    \Return $injection\_indicators$
\EndProcedure
\Statex
\Procedure{IsProcessHollow}{$process$}
    \State $pe\_header \leftarrow$ ExtractPEHeader($process$)
    \State $disk\_pe \leftarrow$ GetDiskPEHeader($process.executable\_path$)
    \Return NOT ComparePESections($pe\_header$, $disk\_pe$)
\EndProcedure
\Statex
\Procedure{IsInjectedCode}{$memory\_region$}
    \If{$memory\_region$.is\_executable AND NOT $memory\_region$.is\_file\_backed AND $memory\_region$.size $> 4096$}
        \State $entropy \leftarrow$ CalculateEntropy($memory\_region$.data)
        \If{$entropy > 7.5$} \Comment{High entropy threshold for packed/encrypted code}
            \Return TRUE
        \EndIf
        \State $api\_calls \leftarrow$ ExtractAPICalls($memory\_region$.data)
        \State $suspicious\_apis \leftarrow$ IdentifySuspiciousAPIs($api\_calls$)
        \Return $|suspicious\_apis| > 3$
    \EndIf
    \Return FALSE
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Memory Dump Collection and Processing}
\label{sec:memory-dump-collection}

The memory dump collection process forms the foundation of our detection framework. This phase involves capturing comprehensive memory snapshots that contain both volatile and persistent memory artifacts essential for malware analysis.

\subsection{Virtualized Environment Configuration}
\label{subsec:environment-config}

Our methodology employs carefully configured virtualized environments to ensure comprehensive malware execution while maintaining analysis system security.

\subsubsection{Hypervisor Selection and Configuration}

We utilize VMware ESXi as our primary hypervisor platform due to its enterprise-grade isolation capabilities and comprehensive snapshot management features. The hypervisor configuration includes:

\begin{itemize}
    \item \textbf{CPU Configuration}: Assignment of multiple CPU cores with hardware virtualization extensions (VT-x/AMD-V) enabled to provide near-native performance.
    
    \item \textbf{Memory Allocation}: Provision of substantial memory resources (minimum 16GB) to prevent memory pressure that could affect malware behavior.
    
    \item \textbf{Storage Configuration}: Utilization of high-performance SSD storage with thick provisioning to minimize I/O bottlenecks.
    
    \item \textbf{Network Isolation}: Implementation of isolated network segments with controlled internet access through proxy servers.
\end{itemize}

Algorithm~\ref{alg:vm-config} demonstrates our automated VM setup process:

\begin{algorithm}[!htbp]
\caption{Automated VM Configuration Script (Pseudocode)}
\label{alg:vm-config}
\begin{algorithmic}[1]
\Procedure{ConfigureVM}{$VM\_NAME$, $MEMORY\_SIZE$, $CPU\_CORES$, $DISK\_SIZE$}
    \State Create VM directory $VM\_PATH$
    \State Generate VMX configuration file:
    \State \quad Set memory size to $MEMORY\_SIZE$
    \State \quad Set CPU cores to $CPU\_CORES$
    \State \quad Disable HGFS, DND, Copy, Paste for anti-detection
    \State \quad Configure network (bridged, vmxnet3)
    \State \quad Configure SCSI disk
    \State Create virtual disk $VM\_NAME.vmdk$ with size $DISK\_SIZE$
    \State Register VM with hypervisor: $vim-cmd solo/registervm$
    \State Log success message
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Guest Operating System Preparation}

The guest operating systems are configured to closely mimic production environments while incorporating specific modifications for malware analysis:

\begin{enumerate}
    \item \textbf{Windows Configuration}:
    \begin{itemize}
        \item Installation of Windows 10/11 with latest security updates
        \item Configuration of realistic user profiles and software installations
        \item Deployment of monitoring tools (Sysmon, Process Monitor, API Monitor)
        \item Installation of common applications (Microsoft Office, browsers, PDF readers)
        \item Configuration of realistic network settings and domain membership
    \end{itemize}
    
    \item \textbf{Linux Configuration}:
    \begin{itemize}
        \item Deployment of Ubuntu/CentOS with standard package installations
        \item Configuration of development environments and common services
        \item Installation of system monitoring tools (strace, ltrace, sysdig)
        \item Setup of containerized environments (Docker, Kubernetes)
    \end{itemize}
\end{enumerate}

\subsection{Memory Dump Acquisition Process}
\label{subsec:dump-acquisition}

The memory dump acquisition process is designed to capture comprehensive memory snapshots while minimizing interference with malware execution.

\subsubsection{Timing and Trigger Mechanisms}

Our methodology employs sophisticated timing mechanisms to capture memory dumps at optimal points during malware execution:

Algorithm~\ref{alg:dump-timing} outlines our intelligent timing strategy.

\begin{algorithm}[!htbp]
\caption{Intelligent Memory Dump Timing}
\label{alg:dump-timing}
\begin{algorithmic}[1]
\Require Malware sample $M$, execution environment $E$
\Ensure Memory dumps $D = \{d_1, d_2, ..., d_n\}$
\State $execution\_time \leftarrow 0$
\State $D \leftarrow \emptyset$
\State $activity\_threshold \leftarrow 0.1$
\State $max\_execution\_time \leftarrow 300$ \Comment{5 minutes maximum}
\State StartExecution($M$, $E$)
\While{$execution\_time < max\_execution\_time$}
    \State $current\_activity \leftarrow$ MonitorSystemActivity()
    \If{$current\_activity > activity\_threshold$}
        \State $dump \leftarrow$ CaptureMemoryDump($E$)
        \State $D \leftarrow D \cup \{dump\}$
        \State WaitForStabilization(30) \Comment{30 second cooldown}
    \EndIf
    \State $network\_activity \leftarrow$ MonitorNetworkActivity()
    \If{DetectC2Communication($network\_activity$)}
        \State $dump \leftarrow$ CaptureMemoryDump($E$)
        \State $D \leftarrow D \cup \{dump\}$
    \EndIf
    \State Sleep(10) \Comment{10 second monitoring interval}
    \State $execution\_time \leftarrow execution\_time + 10$
\EndWhile
\Return $D$
\end{algorithmic}
\end{algorithm}

\subsubsection{Platform-Specific Dump Acquisition}

Our framework implements platform-specific memory acquisition techniques optimized for different operating systems:

\paragraph{Windows Memory Acquisition}

For Windows environments, we utilize Microsoft ProcDump with specific optimizations, as detailed in Algorithm~\ref{alg:windows-dump}:

\begin{algorithm}[!htbp]
\caption{Windows Memory Dump Acquisition (Pseudocode)}
\label{alg:windows-dump}
\begin{algorithmic}[1]
% NOTE: The \Try, \Catch commands are not standard. Define them in your preamble, for example:
% \algdef{SE}[TRY]{Try}{EndTry}{\textbf{try}}{\textbf{end try}}
% \algdef{C}[TRY]{Catch}{...}{\textbf{catch} #1}
\Procedure{CaptureFullMemoryDump}{$output\_directory$, $process\_name$}
    \State Locate ProcDump executable $PROCDUMP\_PATH$
    \State Generate unique $dump\_filename$
    \If{$process\_name$ is specified}
        \State $CMD \leftarrow [PROCDUMP\_PATH, \text{"-ma"}, \text{"-n 1"}, \text{"-s 5"}, process\_name, \text{OUTPUT\_PATH}]$
    \Else
        \State $CMD \leftarrow [PROCDUMP\_PATH, \text{"-ma"}, \text{"-s 10"}, \text{"-n 3"}, \text{OUTPUT\_PATH}]$
    \EndIf
    \State \textbf{try}
        \State Execute $CMD$ with elevated privileges and timeout
        \State Record success, $dump\_path$, $file\_size$, $timestamp$
    \State \textbf{catch} {Exception $e$}
        \State Record failure and $error\_message$
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{CaptureTargetedProcessDump}{$pid$, $output\_directory$}
    \State Locate ProcDump executable $PROCDUMP\_PATH$
    \State Get $process\_name$ from $pid$
    \State Generate unique $dump\_filename$
    \State $CMD \leftarrow [PROCDUMP\_PATH, \text{"-ma"}, \text{"-n 1"}, pid, \text{OUTPUT\_PATH}]$
    \State \textbf{try}
        \State Execute $CMD$ with elevated privileges and timeout
        \State Record success, $dump\_path$, $process\_name$, $pid$, $file\_size$, $timestamp$
    \State \textbf{catch} {Exception $e$}
        \State Record failure and $error\_message$
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{MonitorAndDumpSuspiciousProcesses}{$monitoring\_duration$, $output\_directory$}
    \State $suspicious\_indicators \leftarrow [ \text{'powershell.exe', ..., 'bitsadmin.exe'} ]$
    \State $monitored\_processes \leftarrow \emptyset$
    \State $dumps\_captured \leftarrow \emptyset$
    \State $start\_time \leftarrow$ CurrentTime()
    \While{CurrentTime() - $start\_time < monitoring\_duration$}
        \For{each $proc \in$ EnumerateRunningProcesses()}
            \If{$proc.name$ is in $suspicious\_indicators$ AND $proc.pid$ is NOT in $monitored\_processes$}
                \State Print "Suspicious process detected" message
                \State $dump\_result \leftarrow$ CaptureTargetedProcessDump($proc.pid$, $output\_directory$)
                \If{$dump\_result.success$}
                    \State Add $dump\_result$ to $dumps\_captured$
                    \State Add $proc.pid$ to $monitored\_processes$
                \EndIf
            \EndIf
        \EndFor
        \State Sleep(5) \Comment{Check every 5 seconds}
    \EndWhile
    \Return $dumps\_captured$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Linux Memory Acquisition}

For Linux environments, we implement a stealth-aware memory acquisition system, as shown in Algorithm~\ref{alg:linux-dump}:

\begin{algorithm}[!htbp]
\caption{Linux Stealth Memory Acquisition (Pseudocode)}
\label{alg:linux-dump}
\begin{algorithmic}[1]
\Procedure{CaptureStealthMemoryDump}{$output\_directory$}
    \State Locate LiME kernel module $LIME\_MODULE\_PATH$
    \If{RunningAsRoot()}
        \State \textbf{try}
            \State \Return DirectMemAcquisition($output\_directory$)
        \State \textbf{catch} {Exception $e$}
            \State Log error and Fallback to LiME
        \State \textbf{end try}
    \EndIf
    \Return LiMEMemoryAcquisition($output\_directory$)
\EndProcedure
\Statex
\Procedure{DirectMemAcquisition}{$output\_path$}
    \State Get total system memory $TOTAL\_MEM\_BYTES$ from `/proc/meminfo`
    \State Construct $DD\_COMMAND$ for `/dev/mem` access with stealth parameters (e.g., `bs=1024`, `status=none`)
    \State \textbf{try}
        \State Execute $DD\_COMMAND$
        \State Record success, method, $dump\_path$, $file\_size$, $duration$, $timestamp$
    \State \textbf{catch} {Error $e$}
        \State Throw Runtime Error
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{LiMEMemoryAcquisition}{$output\_path$}
    \State Construct $INSMOD\_COMMAND$ to load LiME kernel module (e.g., `insmod lime.ko path=<output\_path> format=lime compress=0`)
    \State \textbf{try}
        \State Execute $INSMOD\_COMMAND$
        \State Monitor dump progress (e.g., file size growth)
        \State Remove LiME module: `rmmod lime`
        \State Record success, method, $dump\_path$, $file\_size$, $timestamp$
    \State \textbf{catch} {Error $e$}
        \State Throw Runtime Error
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{CaptureProcessMemoryMaps}{$pid$, $output\_directory$}
    \State Read process memory maps from `/proc/$pid$/maps`
    \State Identify readable memory regions ($start\_addr$, $end\_addr$)
    \State Open `/proc/$pid$/mem` for binary read
    \State Create output file `process\_pid\_timestamp.mem`
    \For{each ($start\_addr$, $end\_addr$) in $memory\_regions$}
        \State \textbf{try}
            \State Seek to $start\_addr$ in `/proc/$pid$/mem`
            \State Read region data
            \State Write region data to output file
        \State \textbf{catch} {Error}
            \State Skip inaccessible regions
        \State \textbf{end try}
    \EndFor
    \State Record success, $dump\_path$, $pid$, $memory\_regions$ count, $file\_size$, $timestamp$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Memory Dump to RGB Image Conversion}
\label{sec:rgb-conversion}

The conversion of memory dumps into RGB images represents a critical innovation in our methodology, transforming binary memory data into visual representations that can be analyzed using computer vision techniques. This approach enables the identification of distinctive patterns that differentiate malicious from benign processes.

\subsection{RGB Encoding Rationale and Advantages}
\label{subsec:rgb-rationale}

Our methodology employs RGB encoding instead of traditional grayscale approaches for several fundamental reasons:

\begin{enumerate}
    \item \textbf{Enhanced Information Density}: RGB encoding allows three bytes of memory data to be represented in a single pixel, compared to one byte per pixel in grayscale representations.
    
    \item \textbf{Improved Structural Preservation}: The three-channel representation better preserves the sequential nature of memory data while maintaining spatial relationships.
    
    \item \textbf{Reduced Compression Artifacts}: RGB images with larger width settings result in more compact representations with fewer distortions during subsequent resizing operations.
    
    \item \textbf{Enhanced Pattern Recognition}: The multi-channel nature of RGB images provides richer feature spaces for computer vision algorithms to identify subtle patterns.
\end{enumerate}

Table~\ref{tab:encoding-comparison} presents a comprehensive comparison between grayscale and RGB encoding approaches.

\begin{table}[!htbp]
    \centering
    \caption{Comparison of Grayscale vs RGB Encoding for Memory Visualization}
    \label{tab:encoding-comparison}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Characteristic} & \textbf{Grayscale} & \textbf{RGB} \\
    \hline
    Bytes per pixel & 1 & 3 \\
    Information density & Low & High \\
    Memory efficiency & Moderate & High \\
    Pattern complexity & Simple & Complex \\
    Processing overhead & Low & Moderate \\
    Classification accuracy & Baseline & +3.2\% improvement \\
    \hline
    \end{tabular}
\end{table}

\subsection{Image Rendering Process}
\label{subsec:rendering-process}

The image rendering process involves several sophisticated steps designed to preserve memory structure while creating analyzable visual representations.

Algorithm~\ref{alg:rgb-conversion} outlines our comprehensive RGB conversion methodology.

\begin{algorithm}[!htbp]
\caption{Memory Dump to RGB Image Conversion}
\label{alg:rgb-conversion}
\begin{algorithmic}[1]
\Require Memory dump file $M$, column width $W$, target size $T$
\Ensure RGB image $I_{RGB}$
\State $data \leftarrow$ ReadBinaryFile($M$)
\State $data\_length \leftarrow$ Length($data$)
\State $pixel\_count \leftarrow \lfloor data\_length / 3 \rfloor$
\State $height \leftarrow \lceil pixel\_count / W \rceil$
\State $image\_matrix \leftarrow$ CreateMatrix($height$, $W$, 3)
\For{$i = 0$ to $pixel\_count - 1$}
    \State $r \leftarrow data[3i]$
    \State $g \leftarrow data[3i + 1]$
    \State $b \leftarrow data[3i + 2]$
    \State $x \leftarrow i \bmod W$
    \State $y \leftarrow \lfloor i / W \rfloor$
    \State $image\_matrix[y][x] \leftarrow (r, g, b)$
\EndFor
\If{$data\_length \bmod 3 \neq 0$}
    \State $remaining\_bytes \leftarrow data\_length \bmod 3$
    \State PadWithZeros($image\_matrix$, $remaining\_bytes$)
\EndIf
\State $I_{resized} \leftarrow$ LanczosResize($image\_matrix$, $T$)
\State $I_{RGB} \leftarrow$ SaveAsPNG($I_{resized}$)
\Return $I_{RGB}$
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:rgb-conversion-implementation} demonstrates our optimized RGB conversion process:

\begin{algorithm}[!htbp]
\caption{Optimized RGB Image Conversion Implementation (Pseudocode)}
\label{alg:rgb-conversion-implementation}
\begin{algorithmic}[1]
\Procedure{ConvertMemoryDumpToRGB}{$dump\_path$, $output\_filename$, $column\_width$, $target\_size$}
    \State Log start time
    \State Assert $dump\_path$ exists
    \State $output\_path \leftarrow$ ConstructOutputPath($output\_filename$)
    \State \textbf{try}
        \State $binary\_data \leftarrow$ ReadBinaryDataChunked($dump\_path$)
        \State $rgb\_matrix \leftarrow$ CreateRGBMatrix($binary\_data$, $column\_width$)
        \State $resized\_image \leftarrow$ ResizeWithLanczos($rgb\_matrix$, $target\_size$)
        \State SaveOptimizedPNG($resized\_image$, $output\_path$)
        \State Log success with time and size
        \Return Success result
    \State \textbf{catch} {Exception $e$}
        \State Log error
        \Return Failure result
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{ReadBinaryDataChunked}{$file\_path$, $chunk\_size$}
    \State $data\_chunks \leftarrow \emptyset$
    \State Open $file\_path$ in binary read mode
    \While{TRUE}
        \State $chunk \leftarrow$ Read $chunk\_size$ bytes from file
        \If{NOT $chunk$} \State \textbf{break} \EndIf
        \State Add $chunk$ to $data\_chunks$
    \EndWhile
    \Return Concatenate $data\_chunks$
\EndProcedure
\Statex
\Procedure{CreateRGBMatrix}{$binary\_data$, $column\_width$}
    \State $data\_length \leftarrow$ Length($binary\_data$)
    \If{$data\_length \bmod 3 \neq 0$}
        \State Pad $binary\_data$ with zeros to be divisible by 3
        \State Update $data\_length$
    \EndIf
    \State $pixel\_count \leftarrow data\_length / 3$
    \State $height \leftarrow \lceil pixel\_count / column\_width \rceil$
    \State $rgb\_array \leftarrow$ Convert $binary\_data$ to array of 8-bit integers
    \State $rgb\_matrix \leftarrow$ Reshape $rgb\_array$ to ($pixel\_count$, 3)
    \If{Length($rgb\_matrix$) $<$ $total\_pixels\_needed$}
        \State Pad $rgb\_matrix$ with zeros to fill rectangle
    \EndIf
    \State $image\_matrix \leftarrow$ Reshape $rgb\_matrix$ to ($height$, $column\_width$, 3)
    \Return $image\_matrix$
\EndProcedure
\Statex
\Procedure{ResizeWithLanczos}{$image\_matrix$, $target\_size$}
    \State Convert $image\_matrix$ to PIL Image object (RGB mode)
    \State Resize PIL Image to $target\_size$ using Lanczos resampling
    \State Convert resized PIL Image back to numpy array
    \Return Resized Array
\EndProcedure
\Statex
\Procedure{SaveOptimizedPNG}{$image\_array$, $output\_path$}
    \State Convert $image\_array$ to PIL Image object (RGB mode)
    \State Save PIL Image to $output\_path$ as PNG with optimization and compression
\EndProcedure
\Statex
\Procedure{BatchConvertMemoryDumps}{$dump\_directory$, $max\_workers$}
    \State Get list of $dump\_files$ from $dump\_directory$
    \If{NO $dump\_files$} Log warning and \Return empty list \EndIf
    \State Log batch conversion start
    \State Initialize $results \leftarrow \emptyset$
    \State Create a Process Pool Executor with $max\_workers$
    \For{each $dump\_file$ in $dump\_files$}
        \State Submit ConvertMemoryDumpToRGB task for $dump\_file$ to executor
    \EndFor
    \For{each completed task in executor}
        \State Get $result$ from task
        \State Append $result$ to $results$
        \If{$result.success$} Log success
        \Else
            \State Log error
        \EndIf
    \EndFor
    \State Log batch conversion summary
    \Return $results$
\EndProcedure
\Statex
\Procedure{AnalyzeConversionQuality}{$original\_dump\_path$, $converted\_image\_path$}
    \State $original\_data \leftarrow$ Read $original\_dump\_path$
    \State $converted\_image \leftarrow$ Read and Convert $converted\_image\_path$ to RGB
    \State Calculate $original\_size$ and $converted\_pixels$
    \State $theoretical\_capacity \leftarrow converted\_pixels \times 3$
    \State $original\_entropy \leftarrow$ CalculateEntropy($original\_data$)
    \State $converted\_entropy \leftarrow$ CalculateEntropy(Flattened $converted\_image$)
    \State $compression\_ratio \leftarrow original\_size / theoretical\_capacity$
    \Return Analysis dictionary including entropies and ratios
\EndProcedure
\Statex
\Procedure{CalculateEntropy}{$data$}
    \If{$data$ is numpy array} Convert to bytes \EndIf
    \State $byte\_counts \leftarrow$ Count frequencies of each byte in $data$
    \State $probabilities \leftarrow byte\_counts / \text{Length}(data)$
    \State $entropy \leftarrow -\sum (probabilities \times \log_2(probabilities))$
    \Return $entropy$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Column Width Optimization}
\label{subsec:column-width}

The selection of optimal column width significantly impacts the quality of resulting RGB images and subsequent classification performance. Our methodology evaluates multiple column width schemes through comprehensive experimentation.

\subsubsection{Column Width Schemes Evaluation}

We implement and evaluate four distinct column width approaches:

\begin{enumerate}
    \item \textbf{Fixed 224px Width}: Optimized for compatibility with pre-trained CNN architectures
    \item \textbf{Fixed 300px Width}: Provides higher resolution while maintaining processing efficiency
    \item \textbf{Fixed 4096px Width}: Maximizes data preservation per row, minimizing information loss
    \item \textbf{Square Root Scheme}: Dynamic width calculation based on file size for proportional representation
\end{enumerate}

Algorithm~\ref{alg:column-width-optimization} demonstrates our column width optimization process:

\begin{algorithm}[!htbp]
\caption{Column Width Optimization Analysis (Pseudocode)}
\label{alg:column-width-optimization}
\begin{algorithmic}[1]
\Procedure{EvaluateColumnWidths}{$memory\_dumps$, $labels$, $cv\_folds$}
    \State $results \leftarrow \emptyset$
    \State $width\_schemes \leftarrow [224, 300, 4096, \text{'sqrt'}]$
    \For{each $width\_scheme \in width\_schemes$}
        \State Log current $width\_scheme$
        \State Configure image converter with $width\_scheme$ (dynamic or fixed)
        \State $images \leftarrow \emptyset$
        \For{each $dump\_path \in memory\_dumps$}
            \State \textbf{try}
                \If{$width\_scheme == \text{'sqrt'}$}
                    \State $dynamic\_width \leftarrow$ CalculateSqrtWidth(FileSize($dump\_path$))
                    \State Update converter's $column\_width$ to $dynamic\_width$
                \EndIf
                \State $conversion\_result \leftarrow$ ConvertMemoryDumpToRGB($dump\_path$)
                \If{$conversion\_result.success$}
                    \State Add $conversion\_result.output\_path$ to $images$
                \Else
                    \State Log conversion failure
                \EndIf
            \State \textbf{catch} {Exception $e$}
                \State Log error
            \State \textbf{end try}
        \EndFor
        \If{Conversion success rate is below threshold}
            \State Skip this $width\_scheme$
            \State \textbf{continue} to next $width\_scheme$
        \EndIf
        \State $cv\_scores \leftarrow$ CrossValidateWidthScheme($images$, $labels$, $cv\_folds$)
        \State Store $mean\_accuracy$, $std\_accuracy$, $cv\_scores$, successful conversions in $results[width\_scheme]$
        \State Log $width\_scheme$ performance
    \EndFor
    \Return $results$
\EndProcedure
\Statex
\Procedure{CalculateSqrtWidth}{$file\_size$}
    \Return Integer part of $\sqrt{file\_size / 3}$
\EndProcedure
\Statex
\Procedure{CrossValidateWidthScheme}{$image\_paths$, $labels$, $cv\_folds$}
    \State $features \leftarrow \emptyset$
    \State $valid\_labels \leftarrow \emptyset$
    \For{each $image\_path \in image\_paths$}
        \State \textbf{try}
            \State $feature\_vector \leftarrow$ ExtractCombinedFeatures($image\_path$)
            \State Add $feature\_vector$ to $features$
            \State Add corresponding $label$ to $valid\_labels$
        \State \textbf{catch} {Exception $e$}
            \State Log feature extraction failure
        \State \textbf{end try}
    \EndFor
    \State Convert $features$ and $valid\_labels$ to numpy arrays
    \State Create Stratified K-Fold splitter
    \State $cv\_scores \leftarrow \emptyset$
    \For{each $train\_idx, test\_idx$ in splits}
        \State $X_{train}, X_{test} \leftarrow features[train\_idx], features[test\_idx]$
        \State $y_{train}, y_{test} \leftarrow valid\_labels[train\_idx], valid\_labels[test\_idx]$
        \State Train classifier on $X_{train}, y_{train}$
        \State $y_{pred} \leftarrow$ Predict on $X_{test}$
        \State $accuracy \leftarrow$ CalculateAccuracy($y_{test}, y_{pred}$)
        \State Add $accuracy$ to $cv\_scores$
    \EndFor
    \Return $cv\_scores$
\EndProcedure
\Statex
\Procedure{VisualizeWidthComparison}{$results$}
    \State Create figure with subplots (e.g., 2x2 grid)
    \State Plot bar chart of Mean Accuracy by Column Width
    \State Plot box plot of Cross-Validation Score Distribution
    \State Plot bar chart of Conversion Success Rate
    \State Plot line chart of Accuracy Trend vs Column Width (for numeric widths)
    \State Save figure to file
    \State Show figure
    \Return Generated figure object
\EndProcedure
\Statex
\Procedure{GenerateWidthRecommendation}{$results$}
    \State Define $accuracy\_weight$, $stability\_weight$, $success\_weight$
    \State Initialize $scores \leftarrow \emptyset$
    \For{each $width \in results$}
        \State $accuracy\_score \leftarrow results[width].mean\_accuracy$
        \State $stability\_score \leftarrow 1.0 / (1.0 + results[width].std\_accuracy)$
        \State $success\_score \leftarrow results[width].successful\_conversions / results[width].total\_samples$
        \State $total\_score \leftarrow (accuracy\_weight \times accuracy\_score) + (stability\_weight \times stability\_score) + (success\_weight \times success\_score)$
        \State Store scores in $scores[width]$
    \EndFor
    \State Find $best\_width$ with maximum $total\_score$
    \State Generate human-readable $justification$ for $best\_width$
    \Return Recommendation dictionary including $best\_width$, $score\_breakdown$, $justification$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Feature Extraction Using Image Descriptors}
\label{sec:feature-extraction}

Feature extraction represents a critical component of our methodology, transforming RGB memory images into discriminative numerical representations suitable for machine learning classification. Our approach combines two complementary image descriptors: GIST and Histogram of Oriented Gradients (HOG).

\subsection{GIST Feature Extraction}
\label{subsec:gist-extraction}

The GIST descriptor captures the global spatial structure of an image through multi-scale Gabor filtering, providing a holistic representation of the memory dump's visual characteristics.

\subsubsection{Custom GIST Implementation}

Our custom GIST implementation addresses the limitations of existing libraries while optimizing performance for memory dump analysis, as shown in Algorithm~\ref{alg:gist-implementation}:

\begin{algorithm}[!htbp]
\caption{Custom GIST Feature Extractor Implementation (Pseudocode)}
\label{alg:gist-implementation}
\begin{algorithmic}[1]
\Procedure{InitializeGISTExtractor}{$num\_blocks, num\_scales, num\_orientations, \ldots$}
    \State Set GIST parameters
    \State Pre-compute Gabor kernels based on scales and orientations
    \State Log initialization details
\EndProcedure
\Statex
\Function{CalculateFeatureDimension}{}
    \Return $3 \times (num\_blocks^2) \times num\_orientations \times num\_scales \times 2$ \Comment{3 channels, mean/std}
\EndFunction
\Statex
\Procedure{ExtractFeatures}{$image\_path\_or\_array$}
    \State Log start time
    \State \textbf{try}
        \State Load and Preprocess $image$ (convert to RGB, normalize to $[0,1]$)
        \State $features \leftarrow \emptyset$
        \For{each $channel\_idx \in \{0, 1, 2\}$ (RGB)}
            \State $channel\_image \leftarrow image[:, :, channel\_idx]$
            \State $channel\_features \leftarrow$ ExtractChannelFeatures($channel\_image$)
            \State Add $channel\_features$ to $features$
        \EndFor
        \State Convert $features$ to numpy array
        \State Log completion time and feature dimension
        \Return $features$
    \State \textbf{catch} {Exception $e$}
        \State Log error
        \Return Zero vector of expected dimension
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{ExtractChannelFeatures}{$channel\_image$}
    \State $channel\_features \leftarrow \emptyset$
    \For{each $kernel\_info \in$ PrecomputedGaborKernels}
        \State \textbf{try}
            \State $filtered\_image \leftarrow$ Convolve($channel\_image$, $kernel\_info.kernel$)
            \State $block\_statistics \leftarrow$ ExtractBlockStatistics($filtered\_image$)
            \State Add $block\_statistics$ to $channel\_features$
        \State \textbf{catch} {Exception $e$}
            \State Log warning for failed kernel
            \State Add zero features for failed kernel
        \State \textbf{end try}
    \EndFor
    \Return $channel\_features$
\EndProcedure
\Statex
\Procedure{ExtractBlockStatistics}{$filtered\_image$}
    \State $height, width \leftarrow$ Dimensions($filtered\_image$)
    \State $block\_height \leftarrow height / num\_blocks$
    \State $block\_width \leftarrow width / num\_blocks$
    \State $block\_features \leftarrow \emptyset$
    \For{$i = 0$ to $num\_blocks - 1$}
        \For{$j = 0$ to $num\_blocks - 1$}
            \State Extract $block$ from $filtered\_image$
            \If{$block$ is not empty}
                \State $block\_mean \leftarrow$ Mean($block$)
                \State $block\_std \leftarrow$ StandardDeviation($block$)
                \State Handle NaN/Inf values (set to 0.0)
                \State Add $block\_mean, block\_std$ to $block\_features$
            \Else
                \State Add $[0.0, 0.0]$ to $block\_features$
            \EndIf
        \EndFor
    \EndFor
    \Return $block\_features$
\EndProcedure
\Statex
\Procedure{ExtractFeaturesBatch}{$image\_paths$, $batch\_size$, $n\_jobs$}
    \If{$n\_jobs == 1$}
        \State Process sequentially: For each $img\_path$, call ExtractFeatures
    \Else
        \State Process in parallel using Process Pool Executor
        \State Submit ExtractFeatures tasks for all $image\_paths$
        \State Collect results from futures, handling errors
    \EndIf
    \Return Array of extracted features
\EndProcedure
\Statex
\Procedure{AnalyzeFeatureQuality}{$feature\_vectors$, $labels$}
    \State Convert $feature\_vectors$ to numpy array
    \State Calculate $feature\_dimension$, $num\_samples$
    \State Calculate global $feature\_statistics$ (mean, std, min, max)
    \State Calculate $sparsity$ and $numerical\_stability$
    \If{$labels$ are provided}
        \State Calculate $class\_wise\_feature\_analysis$
        \State Calculate $feature\_separability$ using Fisher's discriminant ratio
    \EndIf
    \Return Analysis dictionary
\EndProcedure
\Statex
\Procedure{CalculateSeparability}{$features$, $labels$}
    \If{Number of unique labels $< 2$} \Return {'note': 'Insufficient classes'} \EndIf
    \State Calculate $overall\_mean$ of features
    \State Initialize $between\_class\_scatter$, $within\_class\_scatter$
    \For{each $label \in$ UniqueLabels}
        \State Get $label\_features$ for current $label$
        \State Calculate $label\_mean$
        \State Update $between\_class\_scatter$ (using $label\_mean$ and $overall\_mean$)
        \State Update $within\_class\_scatter$ (using $label\_features$ and $label\_mean$)
    \EndFor
    \State Calculate $fisher\_ratio \leftarrow$ Diagonal of $between\_class\_scatter / (within\_class\_scatter + \epsilon)$
    \Return Dictionary with $fisher\_ratio$, $fisher\_ratio\_per\_feature$, etc.
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Histogram of Oriented Gradients (HOG) Feature Extraction}
\label{subsec:hog-extraction}

The HOG descriptor complements GIST features by capturing local structural patterns and gradient distributions within memory dump images. This approach focuses on edge patterns and local texture characteristics that may indicate specific malware behaviors.

\subsubsection{Enhanced HOG Implementation}

Our HOG implementation incorporates several optimizations specifically tailored for memory dump analysis, as shown in Algorithm~\ref{alg:hog-implementation}:

\begin{algorithm}[!htbp]
\caption{Enhanced HOG Feature Extractor Implementation (Pseudocode)}
\label{alg:hog-implementation}
\begin{algorithmic}[1]
\Procedure{InitializeHOGExtractor}{$target\_size, cell\_size, block\_size, \ldots$}
    \State Set HOG parameters ($target\_size$, $cell\_size$, $block\_size$, $orientations$, etc.)
    \State Calculate $expected\_dimension$ of HOG features
    \State Log initialization details
\EndProcedure
\Statex
\Function{CalculateHOGDimension}{}
    \State Calculate $cells\_x, cells\_y$
    \State Calculate $blocks\_x, blocks\_y$
    \Return $blocks\_x \times blocks\_y \times block\_size[0] \times block\_size[1] \times orientations$
\EndFunction
\Statex
\Procedure{ExtractFeatures}{$image\_path\_or\_array$, $visualize$}
    \State Log start time
    \State \textbf{try}
        \State Load and convert $image$ to RGB if path provided
        \State $image\_resized \leftarrow$ Resize $image$ to $target\_size$ using Lanczos interpolation
        \State $image\_processed \leftarrow$ PreprocessForHOG($image\_resized$)
        \State $hog\_features, hog\_image \leftarrow$ ComputeHOGFeatures($image\_processed$, $visualize$)
        \State Log completion time and feature dimension
        \Return Dictionary with $hog\_features$, $extraction\_time$, $hog\_image$ (if visualize)
    \State \textbf{catch} {Exception $e$}
        \State Log error
        \Return Dictionary with zero vector features and error
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{PreprocessForHOG}{$image$}
    \State Convert $image$ to grayscale (weighted RGB conversion)
    \State Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to grayscale
    \State Apply optional Gaussian smoothing
    \State Normalize pixel values to $[0, 1]$ range
    \Return Processed image
\EndProcedure
\Statex
\Procedure{ComputeHOGFeatures}{$image$, $visualize$}
    \State \textbf{try}
        \State Compute HOG features using `hog` function with specified parameters
        \If{$visualize$}
            \State Generate $hog\_image$ and rescale intensity for visualization
        \EndIf
        \Return $hog\_features$, $hog\_image$ (or None if not visualize)
    \State \textbf{catch} {Exception $e$}
        \State Log computation failure and re-raise
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{ExtractMultiScaleHOG}{$image\_path\_or\_array$, $scales$}
    \State Initialize $multi\_scale\_features \leftarrow \emptyset$
    \State Load $base\_image$
    \For{each $scale \in scales$}
        \State \textbf{try}
            \State Scale $base\_image$ to $scaled\_size$
            \State Resize $scaled\_image$ back to $target\_size$
            \State $result \leftarrow$ ExtractFeatures($final\_image$)
            \State Add $result.features$ to $multi\_scale\_features$
        \State \textbf{catch} {Exception $e$}
            \State Log warning for failed scale
            \State Add zero features for failed scale
        \State \textbf{end try}
    \EndFor
    \State $combined\_features \leftarrow$ Concatenate $multi\_scale\_features$
    \Return Dictionary with $combined\_features$, $scales\_used$, $feature\_dimension$
\EndProcedure
\Statex
\Procedure{ExtractFeaturesBatch}{$image\_paths$, $batch\_size$, $n\_jobs$}
    \If{$n\_jobs == 1$}
        \State Process sequentially: For each $img\_path$, call ExtractFeatures
    \Else
        \State Process in parallel using Process Pool Executor
        \State Submit ExtractFeatures tasks for all $image\_paths$
        \State Collect results from futures, handling errors
    \EndIf
    \Return Dictionary with $features$, $extraction\_times$, $total\_time$, $average\_time$
\EndProcedure
\Statex
\Procedure{VisualizeHOGFeatures}{$image\_path$, $save\_path$}
    \State $result \leftarrow$ ExtractFeatures($image\_path$, $visualize$=TRUE)
    \If{$result.hog\_image$ is not available} Log error and \Return \EndIf
    \State Create figure with 3 subplots
    \State Plot resized original image
    \State Plot preprocessed grayscale image
    \State Plot HOG features visualization
    \State Save figure to $save\_path$ if specified
    \State Show figure
    \Return Generated figure object
\EndProcedure
\Statex
\Procedure{AnalyzeHOGParameters}{$image\_paths$, $labels$, $parameter\_ranges$}
    \State $results \leftarrow \emptyset$
    \For{each $params \in parameter\_ranges$}
        \State Create temporary HOG Extractor with $params$
        \State \textbf{try}
            \State $features \leftarrow \emptyset$
            \For{each $img\_path \in image\_paths$}
                \State $result \leftarrow$ TempExtractor.ExtractFeatures($img\_path$)
                \State Add $result.features$ to $features$
            \EndFor
            \State Convert $features$ to numpy array
            \State Evaluate classification performance ($mean\_accuracy$, $std\_accuracy$, $cv\_scores$) using cross-validation (e.g., RandomForest)
            \State Store $params$, $mean\_accuracy$, $std\_accuracy$, $feature\_dimension$ in $results[param\_key]$
            \State Log parameter performance
        \State \textbf{catch} {Exception $e$}
            \State Log error for parameter combination
        \State \textbf{end try}
    \EndFor
    \Return $results$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Feature Fusion Methodology}
\label{subsec:feature-fusion}

The combination of GIST and HOG descriptors creates a comprehensive feature representation that captures both global structural patterns and local textural details essential for malware classification.

\subsubsection{Advanced Feature Fusion Implementation}

Our feature fusion approach incorporates several sophisticated techniques to optimize the combination of heterogeneous feature types, as shown in Algorithm~\ref{alg:feature-fusion}:

\begin{algorithm}[!htbp]
\caption{Advanced Feature Fusion Implementation (Pseudocode)}
\label{alg:feature-fusion}
\begin{algorithmic}[1]
\Procedure{InitializeFeatureFusion}{$fusion\_method, normalization\_method, \ldots$}
    \State Set fusion parameters ($fusion\_method$, $normalization\_method$, etc.)
    \State Initialize scalers, PCA reducer, feature selector, fusion weights, fusion network to None
    \State Log initialization details
\EndProcedure
\Statex
\Procedure{FitFusionPipeline}{$gist\_features$, $hog\_features$, $labels$}
    \State Log pipeline fitting start
    \State $gist\_scaler \leftarrow$ CreateScaler()
    \State $hog\_scaler \leftarrow$ CreateScaler()
    \State $gist\_normalized \leftarrow gist\_scaler$.FitAndTransform($gist\_features$)
    \State $hog\_normalized \leftarrow hog\_scaler$.FitAndTransform($hog\_features$)
    \If{$fusion\_method == \text{'concatenation'}$}
        \State $fused\_features \leftarrow$ ConcatenateFeatures($gist\_normalized$, $hog\_normalized$)
    \ElsIf{$fusion\_method == \text{'weighted'}$}
        \State $fused\_features \leftarrow$ WeightedFusion($gist\_normalized$, $hog\_normalized$, $labels$)
    \ElsIf{$fusion\_method == \text{'learned'}$}
        \State $fused\_features \leftarrow$ LearnedFusion($gist\_normalized$, $hog\_normalized$, $labels$)
    \Else
        \State Raise ValueError
    \EndIf
    \If{FeatureSelection is configured}
        \State $fused\_features \leftarrow$ ApplyFeatureSelection($fused\_features$, $labels$)
    \EndIf
    \If{DimensionalityReduction is configured}
        \State $fused\_features \leftarrow$ ApplyDimensionalityReduction($fused\_features$)
    \EndIf
    \State $combined\_scaler \leftarrow$ CreateScaler()
    \State $final\_features \leftarrow combined\_scaler$.FitAndTransform($fused\_features$)
    \State Log pipeline fitted successfully with final dimension
    \Return $final\_features$
\EndProcedure
\Statex
\Procedure{TransformFeatures}{$gist\_features$, $hog\_features$}
    \If{Scalers are not fitted} \State Raise RuntimeError \EndIf
    \State $gist\_normalized \leftarrow gist\_scaler$.Transform($gist\_features$)
    \State $hog\_normalized \leftarrow hog\_scaler$.Transform($hog\_features$)
    \If{$fusion\_method == \text{'concatenation'}$}
        \State $fused\_features \leftarrow$ ConcatenateFeatures($gist\_normalized$, $hog\_normalized$)
    \ElsIf{$fusion\_method == \text{'weighted'}$}
        \State $fused\_features \leftarrow$ ApplyWeightedFusion($gist\_normalized$, $hog\_normalized$)
    \ElsIf{$fusion\_method == \text{'learned'}$}
        \State $fused\_features \leftarrow$ ApplyLearnedFusion($gist\_normalized$, $hog\_normalized$)
    \EndIf
    \If{FeatureSelector is fitted}
        \State $fused\_features \leftarrow feature\_selector$.Transform($fused\_features$)
    \EndIf
    \If{PCAReducer is fitted}
        \State $fused\_features \leftarrow pca\_reducer$.Transform($fused\_features$)
    \EndIf
    \If{CombinedScaler is fitted}
        \State $fused\_features \leftarrow combined\_scaler$.Transform($fused\_features$)
    \EndIf
    \Return $fused\_features$
\EndProcedure
\Statex
\Function{CreateScaler}{}
    \If{$normalization\_method == \text{'standard'}$} \Return StandardScaler()
    \ElsIf{$normalization\_method == \text{'minmax'}$} \Return MinMaxScaler()
    \ElsIf{$normalization\_method == \text{'robust'}$} \Return RobustScaler()
    \Else \State Raise ValueError
    \EndIf
\EndFunction
\Statex
\Function{ConcatenateFeatures}{$gist\_features$, $hog\_features$}
    \Return Horizontally stack $gist\_features$ and $hog\_features$
\EndFunction
\Statex
\Procedure{WeightedFusion}{$gist\_features$, $hog\_features$, $labels$}
    \If{$labels$ is None} Log warning and set equal weights $[0.5, 0.5]$
    \Else
        \State Evaluate $gist\_scores$ and $hog\_scores$ using cross-validation (e.g., RandomForest)
        \State $gist\_weight \leftarrow$ Mean($gist\_scores$)
        \State $hog\_weight \leftarrow$ Mean($hog\_scores$)
        \State Normalize weights: $fusion\_weights \leftarrow [gist\_weight / Total, hog\_weight / Total]$
        \State Log calculated weights
    \EndIf
    \State $weighted\_gist \leftarrow gist\_features \times fusion\_weights[0]$
    \State $weighted\_hog \leftarrow hog\_features \times fusion\_weights[1]$
    \Return Horizontally stack $weighted\_gist$ and $weighted\_hog$
\EndProcedure
\Statex
\Procedure{ApplyWeightedFusion}{$gist\_features$, $hog\_features$}
    \If{$fusion\_weights$ is None} \State Raise RuntimeError \EndIf
    \State Apply $fusion\_weights$ to $gist\_features$ and $hog\_features$
    \Return Horizontally stack weighted features
\EndProcedure
\Statex
\Procedure{LearnedFusion}{$gist\_features$, $hog\_features$, $labels$}
    \If{$labels$ is None} Log warning and fallback to ConcatenateFeatures \EndIf
    \State $concatenated \leftarrow$ Horizontally stack $gist\_features$ and $hog\_features$
    \State Initialize MLPRegressor ($fusion\_network$) as autoencoder-style network
    \State Train $fusion\_network$ to reconstruct $concatenated$ features
    \State Use intermediate layer's prediction as $fused\_features$
    \Return $fused\_features$
\EndProcedure
\Statex
\Procedure{ApplyLearnedFusion}{$gist\_features$, $hog\_features$}
    \If{$fusion\_network$ is not trained} \State Raise RuntimeError \EndIf
    \State $concatenated \leftarrow$ Horizontally stack $gist\_features$ and $hog\_features$
    \Return $fusion\_network$.Predict($concatenated$)
\EndProcedure
\Statex
\Procedure{ApplyFeatureSelection}{$features$, $labels$}
    \If{$labels$ is None} Log warning and \Return original $features$ \EndIf
    \State Get $selection\_method$ and $k\_features$ from config
    \If{$selection\_method == \text{'f\_classif'}$} Initialize SelectKBest with $f\_classif$
    \ElsIf{$selection\_method == \text{'mutual\_info'}$} Initialize SelectKBest with $mutual\_info\_classif$
    \Else \State Raise ValueError
    \EndIf
    \State $feature\_selector \leftarrow$ Fitted SelectKBest
    \State $selected\_features \leftarrow feature\_selector$.FitAndTransform($features$, $labels$)
    \State Log dimension reduction
    \Return $selected\_features$
\EndProcedure
\Statex
\Procedure{ApplyDimensionalityReduction}{$features$}
    \State Get $n\_components$ from config
    \State $pca\_reducer \leftarrow$ PCA(n\_components)
    \State $reduced\_features \leftarrow pca\_reducer$.FitAndTransform($features$)
    \State Log dimension reduction and explained variance ratio
    \Return $reduced\_features$
\EndProcedure
\Statex
\Procedure{SaveFusionPipeline}{$save\_path$}
    \State Serialize and save all fusion components ($gist\_scaler$, $hog\_scaler$, etc.) to $save\_path$ using Pickle
    \State Log save success
\EndProcedure
\Statex
\Procedure{LoadFusionPipeline}{$load\_path$}
    \State Load fusion components from $load\_path$ using Pickle
    \State Restore internal attributes of the object
    \State Log load success
\EndProcedure
\Statex
\Procedure{AnalyzeFusionQuality}{$gist\_features, hog\_features, labels, fused\_features$}
    \State Initialize RandomForestClassifier for evaluation
    \State Evaluate $gist\_scores$, $hog\_scores$, $fused\_scores$ using cross-validation
    \State Calculate mean scores
    \State Calculate percentage improvement of fused over individual features
\Return Analysis dictionary including individual/fused performance and dimension
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Classification Algorithms}
\label{sec:classification-algorithms}

Our methodology employs multiple machine learning algorithms to comprehensively evaluate the effectiveness of our feature extraction and fusion approach. This multi-algorithm evaluation ensures robust performance assessment across different learning paradigms.

\subsection{Multi-Algorithm Classification Framework}
\label{subsec:multi-algorithm}

The classification framework implements five distinct algorithms, each representing different machine learning approaches:

Algorithm~\ref{alg:classification-framework} outlines our comprehensive classification methodology.

\begin{algorithm}[!htbp]
\caption{Multi-Algorithm Classification Framework}
\label{alg:classification-framework}
\begin{algorithmic}[1]
\Require Training features $X_{train}$, training labels $y_{train}$, test features $X_{test}$, test labels $y_{test}$
\Ensure Classification results $R = \{r_1, r_2, ..., r_n\}$
\State $algorithms \leftarrow \{$RandomForest, SMO\_RBF, LinearSVM, XGBoost, J48$\}$
\State $R \leftarrow \emptyset$
\For{each $algorithm \in algorithms$}
    \State $start\_time \leftarrow$ CurrentTime()
    \State $model \leftarrow$ InitializeModel($algorithm$)
    \State $model.fit(X_{train}, y_{train})$
    \State $training\_time \leftarrow$ CurrentTime() - $start\_time$
    \State $y_{pred} \leftarrow model.predict(X_{test})$
    \State $metrics \leftarrow$ CalculateMetrics($y_{test}$, $y_{pred}$)
    \State $result \leftarrow \{$
    \State \quad $algorithm: algorithm,$
    \State \quad $accuracy: metrics.accuracy,$
    \State \quad $precision: metrics.precision,$
    \State \quad $recall: metrics.recall,$
    \State \quad $f1\_score: metrics.f1\_score,$
    \State \quad $training\_time: training\_time,$
    \State \quad $model: model$
    \State $\}$
    \State $R \leftarrow R \cup \{result\}$
\EndFor
\Return $R$
\end{algorithmic}
\end{algorithm}

\subsubsection{Comprehensive Classification Implementation}

Algorithm~\ref{alg:classification-framework-implementation} demonstrates our robust multi-algorithm classification system:

\begin{algorithm}[!htbp]
\caption{Comprehensive Classification Framework Implementation (Pseudocode)}
\label{alg:classification-framework-implementation}
\begin{algorithmic}[1]
\Procedure{InitializeClassificationFramework}{$output\_directory, cv\_folds, random\_state$}
    \State Set output directory, CV folds, random state
    \State Create output directories
    \State Initialize classifiers with optimized parameters (RandomForest, SMO\_RBF, LinearSVM, XGBoost, J48)
    \State Setup logging
    \State Log initialization
\EndProcedure
\Statex
\Procedure{TrainAndEvaluateAll}{$X_{train}, y_{train}, X_{test}, y_{test}, class\_names$}
    \State Initialize $results \leftarrow \emptyset$
    \State Log starting classification
    \For{each $name, classifier \in$ Classifiers}
        \State Log training/evaluation for $name$
        \State \textbf{try}
            \State $result \leftarrow$ TrainAndEvaluateSingle($classifier, X_{train}, y_{train}, X_{test}, y_{test}, name, class\_names$)
            \State Store $result$ in $results[name]$
            \State Log completion for $name$
        \State \textbf{catch} {Exception $e$}
            \State Log error and store failure result
        \State \textbf{end try}
    \EndFor
    \State $analysis \leftarrow$ AnalyzeResults($results$, $class\_names$)
    \State SaveResults($results$, $analysis$)
    \Return $results, analysis$
\EndProcedure
\Statex
\Procedure{TrainAndEvaluateSingle}{$classifier, X_{train}, y_{train}, X_{test}, y_{test}, algorithm\_name, class\_names$}
    \State Log start time
    \State Train $classifier$ on $X_{train}, y_{train}$
    \State $training\_time \leftarrow$ Elapsed time
    \State $y_{pred} \leftarrow classifier$.Predict($X_{test}$)
    \State Calculate basic metrics ($accuracy, precision, recall, f1$)
    \State Generate $confusion\_matrix$
    \State Calculate $cv\_scores$ using cross-validation
    \State Generate detailed $classification\_report$
    \State Extract $feature\_importance$ if available
    \State Get $y_{pred\_proba}$ if classifier supports
    \Return Result dictionary with all metrics and data
\EndProcedure
\Statex
\Procedure{AnalyzeResults}{$results$, $class\_names$}
    \State Initialize $analysis$ dictionary
    \State Populate $performance\_summary$ from $results$
    \State Filter valid results (no errors)
    \If{valid results exist}
        \State Rank algorithms by accuracy, F1 score, CV stability
        \State Store rankings in $analysis['ranking']$
    \EndIf
    \If{Number of valid results $ \ge 2$}
        \State $analysis['statistical\_tests'] \leftarrow$ PerformStatisticalTests(Valid results)
    \EndIf
    \State $analysis['ensemble\_analysis'] \leftarrow$ AnalyzeEnsemblePotential(Valid results)
    \Return $analysis$
\EndProcedure
\Statex
\Procedure{PerformStatisticalTests}{$results$}
    \State Collect $cv\_scores$ for each algorithm
    \State Initialize $pairwise\_tests \leftarrow \emptyset$
    \For{each pair of algorithms $(alg1, alg2)$}
        \State Perform paired t-test on $cv\_scores[alg1]$ vs $cv\_scores[alg2]$
        \State Calculate Cohen's d effect size
        \State Store $t\_statistic, p\_value, cohens\_d, significant$ in $pairwise\_tests$
    \EndFor
    \Return Dictionary with $pairwise\_tests$ and interpretation
\EndProcedure
\Statex
\Procedure{AnalyzeEnsemblePotential}{$results$}
    \State Collect $predictions$ from all algorithms
    \State Calculate $pairwise\_agreement$ between predictions of different algorithms
    \If{Number of algorithms $\ge 3$}
        \State Perform simple majority voting to get $ensemble\_predictions$
        \State Calculate $ensemble\_accuracy$
        \Return Dictionary with $pairwise\_agreement$, $average\_agreement$, $ensemble\_accuracy$, $ensemble\_improvement$
    \Else
        \Return Dictionary with $pairwise\_agreement$ and note about insufficient algorithms
    \EndIf
\EndProcedure
\Statex
\Procedure{VisualizeResults}{$results$, $analysis$}
    \State Create figure with multiple subplots
    \State Plot Accuracy Comparison (bar chart)
    \State Plot F1 Score Comparison (bar chart)
    \State Plot Training Time Comparison (bar chart, log scale)
    \State Plot Cross-Validation Score Distribution (box plot)
    \State Plot Confusion Matrix for Best Algorithm (heatmap)
    \State Plot Performance Radar Chart (for top 3 algorithms)
    \State Plot Performance Heatmap (metrics vs algorithms)
    \State Save figure to file
    \State Show figure
    \Return Generated figure object
\EndProcedure
\Statex
\Procedure{SaveResults}{$results$, $analysis$}
    \State Save detailed $results$ to PKL file
    \State Convert $analysis$ (including numpy arrays) to JSON-serializable format
    \State Save converted $analysis$ to JSON file
    \State Convert performance summary from $analysis$ to Pandas DataFrame
    \State Save DataFrame to CSV file
    \State Log save success
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Unknown Malware Detection Using UMAP}
\label{sec:umap-detection}

The detection of previously unseen malware variants represents one of the most challenging aspects of cybersecurity. Our methodology addresses this challenge through the application of Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction and enhanced feature representation in binary classification tasks.

\subsection{UMAP Theoretical Foundation}
\label{subsec:umap-theory}

UMAP operates on the principle of constructing high-dimensional topological representations and projecting them onto lower-dimensional manifolds while preserving both local and global structure. Unlike traditional dimensionality reduction techniques, UMAP maintains the intrinsic geometry of the data, making it particularly suitable for malware detection where both fine-grained code patterns and broader structural similarities are important.

\subsubsection{Mathematical Foundation}

The UMAP algorithm can be formally described through the following mathematical framework:

Given a dataset $X = \{x_1, x_2, ..., x_n\}$ in high-dimensional space $\mathbb{R}^d$, UMAP constructs a fuzzy topological representation and optimizes a low-dimensional embedding $Y = \{y_1, y_2, ..., y_n\}$ in $\mathbb{R}^k$ where $k \ll d$.

The optimization objective minimizes the cross-entropy between the high-dimensional and low-dimensional fuzzy set representations:

\begin{equation}
C = \sum_{i,j} w_{ij} \log\left(\frac{w_{ij}}{v_{ij}}\right) + (1-w_{ij}) \log\left(\frac{1-w_{ij}}{1-v_{ij}}\right)
\end{equation}

where $w_{ij}$ represents the probability that points $x_i$ and $x_j$ are connected in the high-dimensional space, and $v_{ij}$ represents the probability in the low-dimensional embedding.

\subsection{Binary Classification Framework for Unknown Malware}
\label{subsec:binary-classification}

Our binary classification approach transforms the multi-class malware detection problem into a more generalizable binary classification task that can effectively identify previously unseen malware families.

Algorithm~\ref{alg:binary-classification} outlines our comprehensive binary classification methodology.

\begin{algorithm}[!htbp]
\caption{Binary Classification Framework for Unknown Malware Detection}
\label{alg:binary-classification}
\begin{algorithmic}[1]
\Require Dataset $D$, malware families $F$, benign samples $B$
\Ensure Binary classification performance $P$
\State $folds \leftarrow$ CreateStratifiedFolds($F$, 3) \Comment{3-fold validation}
\State $P \leftarrow \emptyset$
\For{each $fold \in folds$}
    \State $F_{train} \leftarrow$ SelectTrainingFamilies($fold$, 7) \Comment{7 known families}
    \State $F_{test} \leftarrow$ SelectTestFamilies($fold$, 3) \Comment{3 unknown families}
    \State $X_{train}, y_{train} \leftarrow$ PrepareTrainingData($F_{train}$, $B$)
    \State $X_{test}, y_{test} \leftarrow$ PrepareTestData($F_{test}$, $B$)
    \State $X_{train}^{UMAP} \leftarrow$ UMAP.fit\_transform($X_{train}$, $y_{train}$)
    \State $X_{test}^{UMAP} \leftarrow$ UMAP.transform($X_{test}$)
    \For{each $classifier \in \{$RF, SVM, XGBoost$\}$}
        \State $model \leftarrow classifier.fit(X_{train}^{UMAP}, y_{train})$
        \State $y_{pred} \leftarrow model.predict(X_{test}^{UMAP})$
        \State $metrics \leftarrow$ EvaluatePerformance($y_{test}$, $y_{pred}$)
        \State $P \leftarrow P \cup \{$fold, classifier, metrics$\}$
    \EndFor
\EndFor
\Return $P$
\end{algorithmic}
\end{algorithm}

\subsubsection{Advanced UMAP Implementation for Malware Detection}

Our UMAP implementation incorporates several optimizations specifically designed for malware detection scenarios, as shown in Algorithm~\ref{alg:umap-implementation}:

\begin{algorithm}[!htbp]
\caption{Advanced UMAP Implementation for Malware Detection (Pseudocode)}
\label{alg:umap-implementation}
\begin{algorithmic}[1]
\Procedure{InitializeMalwareUMAPDetector}{$n\_neighbors, min\_dist, n\_components, \ldots$}
    \State Set UMAP parameters and output directory
    \State Initialize binary classifiers (RandomForest, LinearSVM, XGBoost)
    \State Setup logging
    \State Log initialization details
\EndProcedure
\Statex
\Procedure{CreateFoldsForUnknownDetection}{$features, labels, family\_labels, n\_folds, known\_families\_per\_fold$}
    \State Identify unique malware families
    \State Get benign samples
    \State Log fold creation details
    \State Initialize $folds \leftarrow \emptyset$
    \State Shuffle malware families
    \For{each $fold\_idx \in 0 \ldots n\_folds-1$}
        \State Select $unknown\_families$ and $known\_families$ for this fold
        \State $fold\_data \leftarrow$ CreateSingleFold($features, labels, family\_labels, known\_families, unknown\_families, fold\_idx$)
        \State Add $fold\_data$ to $folds$
        \State Log fold family details
    \EndFor
    \Return $folds$
\EndProcedure
\Statex
\Procedure{CreateSingleFold}{$features, labels, family\_labels, known\_families, unknown\_families, fold\_idx$}
    \State Get indices for $known\_malware\_samples$ and $unknown\_malware\_samples$
    \State Get indices for $benign\_samples$
    \State Proportionally split $benign\_samples$ into training and testing sets
    \State Create $train\_set \leftarrow$ Known Malware Samples + Training Benign Samples
    \State Create $test\_set \leftarrow$ Unknown Malware Samples + Testing Benign Samples
    \Return Dictionary with fold data ($X_{train}, y_{train}, X_{test}, y_{test}$, etc.)
\EndProcedure
\Statex
\Procedure{EvaluateUnknownMalwareDetection}{$folds$, $use\_umap$}
    \State Initialize $results$ (defaultdict of lists)
    \State Log evaluation start
    \For{each $fold\_data \in folds$}
        \State Get $X_{train}, y_{train}, X_{test}, y_{test}$ from $fold\_data$
        \If{$use\_umap$}
            \State $X_{train\_transformed}, X_{test\_transformed} \leftarrow$ ApplyUMAPTransformation($X_{train}, y_{train}, X_{test}$, Fold Index)
        \Else
            \State Apply standard scaling to $X_{train}, X_{test}$
        \EndIf
        \For{each $clf\_name, classifier \in$ Classifiers}
            \State $fold\_result \leftarrow$ EvaluateSingleClassifier($classifier, X_{train\_transformed}, y_{train}, X_{test\_transformed}, y_{test}, clf\_name$, Fold Index)
            \State Add fold-specific metadata to $fold\_result$
            \State Append $fold\_result$ to $results[clf\_name]$
        \EndFor
    \EndFor
    \State $averaged\_results \leftarrow$ CalculateAveragePerformance($results$)
    \Return $results, averaged\_results$
\EndProcedure
\Statex
\Procedure{ApplyUMAPTransformation}{$X_{train}, y_{train}, X_{test}$, $fold\_idx$}
    \State Log start time
    \State Initialize UMAP reducer with configured parameters and $random\_state$ adjusted by $fold\_idx$
    \State \textbf{try}
        \State $X_{train\_umap} \leftarrow umap\_reducer$.FitAndTransform($X_{train}, y_{train}$)
        \State $X_{test\_umap} \leftarrow umap\_reducer$.Transform($X_{test}$)
        \State Log transformation completion and dimension reduction
        \Return $X_{train\_umap}, X_{test\_umap}$
    \State \textbf{catch} {Exception $e$}
        \State Log UMAP failure and fallback to StandardScaler
        \Return Scaled original features
    \State \textbf{end try}
\EndProcedure
\Statex
\Procedure{EvaluateSingleClassifier}{$classifier, X_{train}, y_{train}, X_{test}, y_{test}, clf\_name, fold\_idx$}
    \State Log start time
    \If{$clf\_name == \text{'XGBoost'}$} Adjust $scale\_pos\_weight$ based on $y_{train}$ distribution \EndIf
    \State Train $classifier$ on $X_{train}, y_{train}$
    \State $training\_time \leftarrow$ Elapsed time
    \State $y_{pred} \leftarrow classifier$.Predict($X_{test}$)
    \State Get $y_{pred\_proba}$ if classifier supports
    \State Calculate $accuracy, precision, recall, f1, auc$ (if probabilities available)
    \State Calculate $class\_specific\_metrics$ (confusion matrix, report)
    \Return Result dictionary with all metrics and predictions
\EndProcedure
\Statex
\Procedure{CalculateClassSpecificMetrics}{$y_{true}, y_{pred}$}
    \State Calculate $confusion\_matrix$
    \State Generate $classification\_report$ (as dict)
    \Return Dictionary with $confusion\_matrix$, $classification\_report$, and individual class metrics (precision, recall, f1)
\EndProcedure
\Statex
\Procedure{CalculateAveragePerformance}{$results$}
    \State Initialize $averaged\_results \leftarrow \emptyset$
    \For{each $clf\_name, fold\_results \in results$}
        \State Filter out failed results
        \If{no valid results} \State Store error for classifier and \textbf{continue} \EndIf
        \State Calculate mean, std, min, max for $accuracy, precision, recall, f1\_score, training\_time$, AUC (if available)
        \State Calculate mean/std for class-specific metrics (benign/malware precision)
        \State Store averages in $averaged\_results[clf\_name]$
    \EndFor
    \Return $averaged\_results$
\EndProcedure
\Statex
\Procedure{CompareWithWithoutUMAP}{$folds$}
    \State Log comparison start
    \State $results\_no\_umap, avg\_no\_umap \leftarrow$ EvaluateUnknownMalwareDetection($folds$, $use\_umap$=FALSE)
    \State $results\_with\_umap, avg\_with\_umap \leftarrow$ EvaluateUnknownMalwareDetection($folds$, $use\_umap$=TRUE)
    \State Initialize $improvements \leftarrow \emptyset$
    \For{each $clf\_name \in$ classifiers}
        \If{both UMAP and No-UMAP results exist}
            \State Calculate $improvement\_percentage$ and $absolute\_improvement$ in accuracy
            \State Store results in $improvements[clf\_name]$
        \EndIf
    \EndFor
    \Return Comparison dictionary including all results and improvements
\EndProcedure
\Statex
\Procedure{OptimizeUMAPParameters}{$fold\_data$, $param\_grid$}
    \State Get $X_{train}, y_{train}, X_{test}, y_{test}$ from $fold\_data$
    \State Initialize $best\_params \leftarrow \text{None}$, $best\_score \leftarrow 0$, $results \leftarrow \emptyset$
    \State Log optimization start
    \For{each $params \in$ all combinations in $param\_grid$}
        \State \textbf{try}
            \State Initialize UMAP reducer with $params$
            \State $X_{train\_umap} \leftarrow$ UMAP.FitAndTransform($X_{train}, y_{train}$)
            \State $X_{test\_umap} \leftarrow$ UMAP.Transform($X_{test}$)
            \State Train a fast classifier (e.g., RandomForest) on UMAP-transformed data
            \State $accuracy \leftarrow$ CalculateAccuracy(True labels, Predicted labels)
            \State Store $params, accuracy, transformed\_shape$ in $results$
            \If{$accuracy > best\_score$} Update $best\_score, best\_params$ \EndIf
            \State Log parameter performance
        \State \textbf{catch} {Exception $e$}
            \State Log warning for failed parameter combination
        \State \textbf{end try}
    \EndFor
    \Return Dictionary with $best\_params, best\_score$, and all results
\EndProcedure
\Statex
\Procedure{VisualizeUMAPEmbeddings}{$features, labels, family\_labels, title$}
    \State Create 2D UMAP reducer (for visualization purposes)
    \State $embedding\_2d \leftarrow$ UMAP.FitAndTransform($features, labels$)
    \State Create figure for plotting
    \If{$family\_labels$ is provided}
        \State Scatter plot points colored by $family\_labels$
    \Else
        \State Scatter plot points colored by binary $labels$ (Benign/Malware)
    \EndIf
    \State Set title, labels, legend
    \State Save plot to file
    \State Show plot
    \Return $embedding\_2d$
\EndProcedure
\Statex
\Procedure{GenerateComprehensiveReport}{$comparison\_results$}
    \State Initialize $report\_lines \leftarrow \emptyset$
    \State Add report header
    \State Add Performance Summary section for each classifier, detailing accuracy with/without UMAP and improvement
    \State Add Best Performing Algorithm summary
    \State Add UMAP Configuration details
    \State Save report to Markdown file
    \State Log report save success
    \Return Generated report content as string
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Performance Evaluation Methodologies}
\label{subsec:performance-evaluation}

Our evaluation methodology employs comprehensive metrics and statistical analysis techniques to ensure robust assessment of the proposed malware detection framework.

\subsubsection{Evaluation Metrics Framework}

The evaluation framework incorporates multiple complementary metrics to provide a holistic view of detection performance, as shown in Algorithm~\ref{alg:performance-evaluation}:

\begin{algorithm}[!htbp]
\caption{Comprehensive Performance Evaluation Framework (Pseudocode)}
\label{alg:performance-evaluation}
\begin{algorithmic}[1]
\Procedure{InitializePerformanceEvaluator}{$output\_directory, class\_names$}
    \State Set output directory and class names
    \State Create output directory
    \State Setup logging
\EndProcedure
\Statex
\Procedure{EvaluateClassificationPerformance}{$y_{true}, y_{pred}, y_{pred\_proba}, algorithm\_name$}
    \State Log evaluation start for $algorithm\_name$
    \State Initialize $evaluation\_results$ dictionary
    \State $evaluation\_results['basic\_metrics'] \leftarrow$ CalculateBasicMetrics($y_{true}, y_{pred}$)
    \State $evaluation\_results['advanced\_metrics'] \leftarrow$ CalculateAdvancedMetrics($y_{true}, y_{pred}$)
    \State $evaluation\_results['confusion\_matrix\_analysis'] \leftarrow$ AnalyzeConfusionMatrix($y_{true}, y_{pred}$)
    \State $evaluation\_results['class\_specific\_metrics'] \leftarrow$ CalculateClassSpecificMetrics($y_{true}, y_{pred}$)
    \If{$y_{pred\_proba}$ is not None}
        \State $evaluation\_results['probability\_metrics'] \leftarrow$ CalculateProbabilityMetrics($y_{true}, y_{pred\_proba}$)
        \State $evaluation\_results['threshold\_analysis'] \leftarrow$ AnalyzeDecisionThresholds($y_{true}, y_{pred\_proba}$)
    \EndIf
    \State $evaluation\_results['statistical\_analysis'] \leftarrow$ PerformStatisticalAnalysis($y_{true}, y_{pred}$)
    \Return $evaluation\_results$
\EndProcedure
\Statex
\Function{CalculateBasicMetrics}{$y_{true}, y_{pred}$}
    \State Calculate $accuracy\_score$
    \State Calculate $precision, recall, f1\_score$ (weighted average)
    \Return Dictionary with these metrics
\EndFunction
\Statex
\Function{CalculateAdvancedMetrics}{$y_{true}, y_{pred}$}
    \State Calculate Matthews Correlation Coefficient ($mcc$)
    \State Calculate Cohen's Kappa ($kappa$)
    \If{Binary classification}
        \State Calculate $tn, fp, fn, tp$ from confusion matrix
        \State Calculate $sensitivity, specificity, balanced\_accuracy, geometric\_mean$
        \State Calculate True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate
    \Else \Comment{Multi-class}
        \State Calculate $balanced\_accuracy$ for multi-class
    \EndIf
    \Return Dictionary with advanced metrics
\EndFunction
\Statex
\Function{AnalyzeConfusionMatrix}{$y_{true}, y_{pred}$}
    \State Calculate raw $confusion\_matrix$ ($cm$)
    \State Normalize $cm$
    \State Calculate $per\_class\_accuracy$
    \State Calculate $misclassification\_rates$
    \Return Dictionary with $cm$, normalized $cm$, per-class accuracy, total samples, correctly/misclassified counts, class distribution, and binary-specific rates (if binary)
\EndFunction
\Statex
\Function{CalculateClassSpecificMetrics}{$y_{true}, y_{pred}$}
    \State Calculate $precision, recall, f1\_score, support$ for each class (average=None)
    \For{each unique class}
        \State Create dictionary for class with its metrics
    \EndFor
    \Return Dictionary of class metrics
\EndFunction
\Statex
\Function{CalculateProbabilityMetrics}{$y_{true}, y_{pred\_proba}$}
    \State Extract $y_{scores}$ (probabilities for positive class in binary or all classes in multi-class)
    \If{Binary classification}
        \State Calculate ROC curve ($fpr, tpr, roc\_thresholds$) and AUC
        \State Calculate Precision-Recall curve ($precision\_curve, recall\_curve, pr\_thresholds$) and AUC
        \Return Dictionary with AUCs and curve data
    \Else
        \Return {'note': 'Probability metrics only available for binary classification'}
    \EndIf
\EndFunction
\Statex
\Function{AnalyzeDecisionThresholds}{$y_{true}, y_{pred\_proba}$}
    \If{Not binary classification} \Return note \EndIf
    \State Extract $y_{scores}$
    \State Iterate $thresholds$ from 0 to 1
    \For{each $threshold$}
        \State $y_{pred\_thresh} \leftarrow (y_{scores} \ge threshold)$
        \If{predictions are not uniform}
            \State Calculate $accuracy, precision, recall, f1$ for this threshold
            \State Append to $threshold\_metrics$ list
        \EndIf
    \EndFor
    \State Find $optimal\_thresholds$ for accuracy, f1, precision, recall
    \Return Dictionary with $threshold\_analysis$ and $optimal\_thresholds$
\EndFunction
\Statex
\Function{PerformStatisticalAnalysis}{$y_{true}, y_{pred}$}
    \State Calculate Chi-square test for independence on confusion matrix ($chi2, p\_value, \ldots$)
    \State Calculate Cramer's V effect size
    \State Calculate $observed\_agreement$ (accuracy)
    \State Calculate $expected\_agreement\_by\_chance$
    \State Calculate $agreement\_beyond\_chance$
    \Return Dictionary with chi-square test results and agreement analysis
\EndFunction
\Statex
\Procedure{CompareAlgorithms}{$evaluation\_results\_list$}
    \State Initialize $comparison\_data \leftarrow \emptyset$
    \For{each $results \in evaluation\_results\_list$}
        \State Extract $algorithm$ name, basic metrics, advanced metrics
        \State Create $comparison\_entry$ with key metrics (Accuracy, F1, MCC, Kappa, Balanced Accuracy, ROC AUC)
        \State Append $comparison\_entry$ to $comparison\_data$
    \EndFor
    \State Create Pandas DataFrame from $comparison\_data$
    \State Save DataFrame to CSV file
    \State Generate rankings by various metrics (Accuracy, F1-Score, MCC, Balanced\_Accuracy)
    \Return Dictionary with $comparison\_table$, $rankings$, $summary\_statistics$
\EndProcedure
\Statex
\Procedure{VisualizePerformanceComparison}{$evaluation\_results\_list$}
    \State Create figure with multiple subplots
    \State Extract data (algorithms, accuracies, f1\_scores, etc.)
    \State Plot Accuracy Comparison (bar chart)
    \State Plot F1-Score Comparison (bar chart)
    \State Plot Precision vs Recall (scatter plot)
    \State Plot Matthews Correlation Coefficient (bar chart)
    \State Plot Performance Radar Chart (for top 3 algorithms)
    \State Plot Performance Heatmap (metrics vs algorithms)
    \State Save figure to file
    \State Show figure
    \Return Generated figure object
\EndProcedure
\Statex
\Procedure{GenerateEvaluationReport}{$evaluation\_results$}
    \State Initialize $report\_lines \leftarrow \emptyset$
    \State Add report header
    \State Add Performance Summary (Accuracy, Precision, Recall, F1-Score)
    \State Add Advanced Metrics (MCC, Kappa, Balanced Accuracy, Sensitivity, Specificity, FPR, FNR)
    \State Add Confusion Matrix Analysis
    \State Add Class-Specific Performance for each class
    \State Add Statistical Analysis (Chi-square, Agreement)
    \State Save report to Markdown file
    \Return Generated report content as string
\EndProcedure
\end{algorithmic}
\end{algorithm}

This comprehensive methodology section provides detailed implementations for:

\begin{enumerate}
    \item \textbf{UMAP-based Unknown Malware Detection}: Advanced dimensionality reduction for enhanced pattern recognition
    \item \textbf{Binary Classification Framework}: Sophisticated approach to detect previously unseen malware families
    \item \textbf{Performance Evaluation}: Comprehensive metrics and statistical analysis for robust assessment
    \item \textbf{Fold Creation Strategy}: Systematic approach to simulate real-world unknown malware scenarios
    \item \textbf{Comparative Analysis}: Methods to evaluate improvements achieved through UMAP transformation
\end{enumerate}

The methodology ensures reproducible results through systematic experimental design while addressing the critical challenge of detecting unknown malware variants in memory-dependent environments.

\section{Experimental Design and Validation Framework}
\label{sec:experimental-design}

Our experimental design follows rigorous scientific principles to ensure reproducible, unbiased, and statistically significant results. The validation framework incorporates multiple validation strategies to comprehensively assess the effectiveness of our proposed malware detection methodology.

\subsection{Experimental Design Principles}
\label{subsec:design-principles}

The experimental design adheres to the following core principles:

\begin{enumerate}
    \item \textbf{Reproducibility}: All experiments are designed with fixed random seeds, documented parameters, and version-controlled implementations
    \item \textbf{Statistical Rigor}: Appropriate sample sizes, significance testing, and confidence intervals
    \item \textbf{Bias Minimization}: Stratified sampling, cross-validation, and blind evaluation procedures
    \item \textbf{Comprehensive Evaluation}: Multiple metrics, baseline comparisons, and ablation studies
\end{enumerate}

Algorithm~\ref{alg:experimental-design} outlines our systematic experimental validation approach.

\begin{algorithm}[!htbp]
\caption{Systematic Experimental Design Framework}
\label{alg:experimental-design}
\begin{algorithmic}[1]
\Require Dataset $D$, methodologies $M$, evaluation metrics $E$
\Ensure Experimental results $R$ with statistical significance
\State $R \leftarrow \emptyset$
\State $baseline\_methods \leftarrow$ InitializeBaselines()
\State $proposed\_methods \leftarrow$ InitializeProposedMethods()
\For{each $methodology \in M$}
    \For{$iteration = 1$ to $n_{iterations}$}
        \State $seed \leftarrow base\_seed + iteration$
        \State SetRandomSeed($seed$)
        \State $train\_set, test\_set \leftarrow$ StratifiedSplit($D$, $seed$)
        \State $results \leftarrow$ EvaluateMethodology($methodology$, $train\_set$, $test\_set$, $E$)
        \State $results.iteration \leftarrow iteration$
        \State $results.seed \leftarrow seed$
        \State $R \leftarrow R \cup \{results\}$
    \EndFor
\EndFor
\State $statistical\_analysis \leftarrow$ PerformStatisticalTests($R$)
\State $significance\_report \leftarrow$ GenerateSignificanceReport($statistical\_analysis$)
\Return $R$, $statistical\_analysis$, $significance\_report$
\end{algorithmic}
\end{algorithm}

\subsubsection{Comprehensive Experimental Validation Implementation}

Algorithm~\ref{alg:experimental-validation-implementation} demonstrates the comprehensive experimental validation framework:

\begin{algorithm}[!htbp]
\caption{Comprehensive Experimental Validation Framework (Pseudocode)}
\label{alg:experimental-validation-implementation}
\begin{algorithmic}[1]
\Procedure{InitializeExperimentalValidationFramework}{$base\_output\_dir, random\_state, n\_iterations$}
    \State Set base output directory, random state, and number of iterations
    \State Create organized subdirectories (results, plots, logs, models)
    \State Setup comprehensive logging
    \State Initialize $experiment\_registry$, $baseline\_methods$, $proposed\_methods$
    \State Log initialization details
\EndProcedure
\Statex
\Procedure{RegisterBaselineMethod}{$name, method\_class, parameters$}
    \State Store $method\_class$ and $parameters$ under $name$ as 'baseline'
    \State Log registration
\EndProcedure
\Statex
\Procedure{RegisterProposedMethod}{$name, method\_class, parameters$}
    \State Store $method\_class$ and $parameters$ under $name$ as 'proposed'
    \State Log registration
\EndProcedure
\Statex
\Procedure{DesignStratifiedExperiment}{$dataset, labels, test\_size, validation\_strategy, cv\_folds$}
    \State Initialize $experimental\_design$ dictionary
    \State Populate $dataset\_info$
    \State Initialize $splits \leftarrow \emptyset$
    \If{$validation\_strategy == \text{'holdout'}$}
        \For{each $iteration \in 0 \ldots n\_iterations-1$}
            \State $current\_seed \leftarrow random\_state + iteration$
            \State Perform $train\_test\_split$ on $dataset, labels$ with $stratify=labels$ and $current\_seed$
            \State Store split info ($X_{train}, X_{test}, y_{train}, y_{test}$, sizes, distributions) in $splits$
        \EndFor
    \ElsIf{$validation\_strategy == \text{'cross\_validation'}$}
        \For{each $iteration \in 0 \ldots n\_iterations-1$}
            \State $current\_seed \leftarrow random\_state + iteration$
            \State Create Stratified K-Fold splitter with $cv\_folds$ and $current\_seed$
            \For{each $fold\_idx, (train\_idx, test\_idx)$ from splitter}
                \State Extract $X_{train}, X_{test}, y_{train}, y_{test}$
                \State Store fold info ($train\_idx, test\_idx$, data) in $cv\_splits$
            \EndFor
            \State Store $cv\_splits$ for current iteration in $splits$
        \EndFor
    \EndIf
    \State Save $experimental\_design$ to JSON file (excluding raw data)
    \State Log completion
    \Return $experimental\_design$
\EndProcedure
\Statex
\Procedure{ExecuteComprehensiveEvaluation}{$experimental\_design$}
    \State $all\_methods \leftarrow$ Combine baseline and proposed methods
    \If{no methods} \State Raise ValueError \EndIf
    \State Log evaluation start
    \State Initialize $experimental\_results$ dictionary
    \For{each $method\_name, method\_info \in all\_methods$}
        \State Log current method
        \State $method\_results \leftarrow$ EvaluateSingleMethod($method\_name, method\_info, experimental\_design$)
        \State Store $method\_results$ in $experimental\_results$
        \State Save intermediate results to file
    \EndFor
    \State Update $execution\_metadata$ (end time, duration)
    \State $statistical\_analysis \leftarrow$ PerformComprehensiveStatisticalAnalysis($experimental\_results['method\_results']$)
    \State Store $statistical\_analysis$ in $experimental\_results$
    \State Save complete $experimental\_results$ to file
    \Return $experimental\_results$
\EndProcedure
\Statex
\Procedure{EvaluateSingleMethod}{$method\_name, method\_info, experimental\_design$}
    \State Initialize $method\_results \leftarrow \emptyset$
    \For{each $split\_info \in experimental\_design.splits$}
        \State Log current split
        \If{$split\_info$ contains 'cv\_folds'}
            \State Initialize $cv\_results \leftarrow \emptyset$
            \For{each $fold\_info \in split\_info.cv\_folds$}
                \State $fold\_result \leftarrow$ EvaluateMethodOnSplit($method\_name, method\_info, fold\_info.data, split\_info.random\_seed$)
                \State Add fold index to $fold\_result$
                \State Append $fold\_result$ to $cv\_results$
            \EndFor
            \State $aggregated\_result \leftarrow$ AggregateCVResults($cv\_results$)
            \State Add split metadata to $aggregated\_result$
            \State Append $aggregated\_result$ to $method\_results$
        \Else \Comment{Holdout split}
            \State $split\_result \leftarrow$ EvaluateMethodOnSplit($method\_name, method\_info, split\_info.data, split\_info.random\_seed$)
            \State Add split metadata to $split\_result$
            \State Append $split\_result$ to $method\_results$
        \EndIf
    \EndFor
    \Return $method\_results$
\EndProcedure
\Statex
\Procedure{EvaluateMethodOnSplit}{$method\_name, method\_info, data\_split, random\_seed$}
    \State Log start time
    \State \textbf{try}
        \State Initialize $method\_instance$ from $method\_class$ with $parameters$ and $random\_seed$
        \State Extract $X_{train}, X_{test}, y_{train}, y_{test}$ from $data\_split$
        \State Train $method\_instance$ (fit)
        \State $training\_time \leftarrow$ Elapsed time
        \State Make $y_{pred} \leftarrow method\_instance$.Predict($X_{test}$)
        \State $prediction\_time \leftarrow$ Elapsed time
        \State Get $y_{pred\_proba}$ if available
        \State $metrics \leftarrow$ CalculateComprehensiveMetrics($y_{test}, y_{pred}, y_{pred\_proba}$)
        \State $complexity\_metrics \leftarrow$ AnalyzeModelComplexity($method\_instance$)
        \State Build $result$ dictionary with success, metrics, timing, data info
        \If{Save model is true} Save $method\_instance$ to file and add $model\_path$ to result \EndIf
        \Return $result$
    \State \textbf{catch} {Exception $e$}
        \State Log error
        \Return Failure result with error message
    \State \textbf{end try}
\EndProcedure
\Statex
\Function{CalculateComprehensiveMetrics}{$y_{true}, y_{pred}, y_{pred\_proba}$}
    \State Calculate $accuracy, precision, recall, f1\_score$ (weighted)
    \If{$y_{pred\_proba}$ is not None and binary classification}
        \State Calculate ROC AUC and Average Precision Score
    \EndIf
    \Return All calculated metrics
\EndFunction
\Statex
\Function{AnalyzeModelComplexity}{$model\_instance$}
    \State Initialize $complexity\_metrics \leftarrow \emptyset$
    \State Add number of input features
    \State Add tree-based specific metrics ($n\_estimators, max\_depth$)
    \State Add SVM specific metrics ($n\_support\_vectors$)
    \State Add neural network specific metrics ($hidden\_layer\_sizes$)
    \State Add feature importance statistics (mean, std, max, zero count) if available
    \Return $complexity\_metrics$
\EndFunction
\Statex
\Procedure{AggregateCVResults}{$cv\_results$}
    \State Filter successful results
    \If{no successful results} \Return failure \EndIf
    \State Aggregate $metrics$ (mean, std, min, max for each metric)
    \State Aggregate $timing$ (mean, std for training/prediction time)
    \Return Aggregated result with success status
\EndProcedure
\Statex
\Procedure{PerformComprehensiveStatisticalAnalysis}{$method\_results$}
    \State Log statistical analysis start
    \State Extract performance data (e.g., accuracy) for each method from results
    \State Initialize $statistical\_tests \leftarrow \emptyset$
    \For{each pair of methods $(method1, method2)$}
        \If{enough data for both methods}
            \State Perform Paired t-test on their performance data
            \State Calculate Cohen's d effect size
            \State Store test results ($t\_statistic, p\_value, cohens\_d, significant$, interpretation)
        \EndIf
    \EndFor
    \If{more than 2 methods}
        \State Perform Overall ANOVA test
    \EndIf
    \State Calculate summary statistics (mean, std, median, min, max, n\_experiments) for each method's performance
    \Return Dictionary with pairwise tests, ANOVA, summary statistics, and raw performance data
\EndProcedure
\Statex
\Function{InterpretEffectSize}{$cohens\_d$}
    \If{$cohens\_d < 0.2$} \Return "negligible"
    \ElsIf{$cohens\_d < 0.5$} \Return "small"
    \ElsIf{$cohens\_d < 0.8$} \Return "medium"
    \Else \Return "large"
    \EndIf
\EndFunction
\Statex
\Procedure{SaveIntermediateResults}{$method\_name, results$}
    \State Convert numpy objects in $results$ to JSON-serializable format
    \State Save $results$ to JSON file
\EndProcedure
\Statex
\Procedure{SaveCompleteResults}{$experimental\_results$}
    \State Convert numpy objects in $experimental\_results$ to JSON-serializable format
    \State Save $experimental\_results$ to JSON file
    \State Log save success
\EndProcedure
\Statex
\Function{ConvertNumpyForJSON}{$obj, exclude\_data$}
    \If{$obj$ is dictionary} Recursively convert values, exclude 'data' if requested
    \ElsIf{$obj$ is list} Recursively convert items
    \ElsIf{$obj$ is numpy array} Convert to list
    \ElsIf{$obj$ is numpy scalar} Convert to Python scalar
    \ElsIf{$obj$ is numpy boolean} Convert to Python boolean
    \Else \Return $obj$
    \EndIf
\EndFunction
\Statex
\Procedure{GenerateExperimentalReport}{$experimental\_results$}
    \State Initialize $report\_lines \leftarrow \emptyset$
    \State Add report header
    \State Add Experimental Setup section (total samples, test size, strategy, iterations, class distribution)
    \State Add Performance Summary table (Mean, Std, Min, Max Accuracy per method)
    \State Add Statistical Significance Analysis section (pairwise t-tests with p-values and effect sizes)
    \State Add ANOVA Test Results (if performed)
    \State Add Best Performing Method summary
    \State Add Execution Information (total duration, methods evaluated)
    \State Save report to Markdown file
    \State Log report save success
    \Return Generated report content as string
\EndProcedure
\Statex
\Procedure{VisualizeExperimentalResults}{$experimental\_results$}
    \State Get statistical analysis and performance data
    \State Create figure with multiple subplots
    \State Plot Performance Comparison with Standard Deviation (bar chart)
    \State Plot Performance Distribution Box Plot
    \State Plot Performance Across Experimental Iterations (line plot)
    \State Plot Statistical Significance Matrix (heatmap)
    \State Save figure to file
    \State Show figure
    \State Log save success
    \Return Generated figure object
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Dataset Preparation and Preprocessing Methodologies}
\label{sec:dataset-preparation}

The quality and representativeness of the dataset directly impact the effectiveness of our malware detection framework. This section details our comprehensive approach to dataset preparation, including collection strategies, preprocessing techniques, and quality assurance measures.

\subsection{Dataset Collection Strategy}
\label{subsec:dataset-collection}

Our dataset collection strategy prioritizes diversity, authenticity, and ethical considerations while ensuring comprehensive coverage of modern malware families and attack vectors.

\subsubsection{Comprehensive Dataset Collection Framework}

Algorithm~\ref{alg:dataset-collection} illustrates the comprehensive dataset collection and management system:

\begin{algorithm}[!htbp]
\caption{Comprehensive Dataset Collection and Management (Pseudocode)}
\label{alg:dataset-collection}
\begin{algorithmic}[1]
\Procedure{InitializeMalwareDatasetManager}{$base\_directory, database\_path$}
    \State Set base directory and create organized subdirectories (raw\_samples, processed\_samples, etc.)
    \State Initialize SQLite database for metadata tracking
    \State Setup logging
    \State Initialize sample tracking variables
\EndProcedure
\Statex
\Procedure{RegisterSample}{$file\_path, family\_label, source, pe\_info, static\_analysis$}
    \State Assert $file\_path$ exists
    \State $file\_hash \leftarrow$ CalculateFileHash($file\_path$)
    \If{SampleExists($file\_hash$)} Log warning and \Return $file\_hash$ \EndIf
    \State $file\_info \leftarrow$ AnalyzeFileProperties($file\_path$)
    \State Create family-specific directory for raw samples
    \State Construct $new\_filename$ and $destination\_path$
    \State \textbf{try}
        \State Copy $file\_path$ to $destination\_path$
        \State Insert sample metadata into database (file\_hash, filename, family, size, type, etc.)
        \State UpdateFamilyStatistics($family\_label$, $file\_info$)
        \State Log operation (sample registration) as success
        \Return $file\_hash$
    \State \textbf{catch} {Exception $e$}
        \State Log error and operation as failure
        \State Re-raise exception
    \State \textbf{end try}
\EndProcedure
\Statex
\Function{CalculateFileHash}{$file\_path$}
    \State Compute SHA-256 hash of $file\_path$ in chunks
    \Return Hex digest of hash
\EndFunction
\Statex
\Function{SampleExists}{$file\_hash$}
    \State Query database for $file\_hash$
    \Return TRUE if count $> 0$, FALSE otherwise
\EndFunction
\Statex
\Function{AnalyzeFileProperties}{$file\_path$}
    \State Get file size, creation/modification times
    \State Determine file type (e.g., PE, ELF) by magic bytes
    \Return Dictionary with file properties
\EndFunction
\Statex
\Procedure{UpdateFamilyStatistics}{$family\_label, file\_info$}
    \State Check if $family\_label$ exists in database
    \If{exists}
        \State Update $sample\_count$, $avg\_file\_size$, $last\_updated$ for existing family
    \Else
        \State Insert new family entry with $sample\_count=1$, $avg\_file\_size=$ $file\_info.size$
    \EndIf
    \State Commit database changes
\EndProcedure
\Statex
\Procedure{LogOperation}{$sample\_hash, operation, success, error\_message, duration$}
    \State Insert log entry into `processing\_log` table in database
    \State Commit database changes
\EndProcedure
\Statex
\Procedure{BulkImportFromDirectory}{$source\_directory, family\_mapping, source\_name$}
    \State Assert $source\_directory$ exists
    \State Initialize $import\_stats$ (total processed, successful, duplicates, errors, families processed)
    \State Log bulk import start
    \For{each $subdir \in source\_directory$}
        \If{$subdir$ is directory}
            \State Map $subdir.name$ to $family\_label$
            \State Log processing family
            \For{each $file\_path \in subdir$}
                \If{$file\_path$ is file}
                    \State Increment $total\_processed$
                    \State \textbf{try}
                        \State $file\_hash \leftarrow$ CalculateFileHash($file\_path$)
                        \If{SampleExists($file\_hash$)}
                            \State Increment $duplicates\_skipped$ and \textbf{continue}
                        \EndIf
                        \State RegisterSample($file\_path, family\_label, source\_name$)
                        \State Increment $successful\_imports$ and $families\_processed[family\_label]$
                    \State \textbf{catch} {Exception $e$}
                        \State Increment $errors$ and Log error
                    \State \textbf{end try}
                \EndIf
            \EndFor
        \EndIf
    \EndFor
    \State Log bulk import summary
    \Return $import\_stats$
\EndProcedure
\Statex
\Procedure{ValidateDatasetIntegrity}{}
    \State Log validation start
    \State Initialize $validation\_results$ dictionary
    \State Query all samples from database (file\_hash, filename, family\_label)
    \For{each sample record}
        \State Construct $expected\_path$ to raw sample file
        \If{$expected\_path$ does not exist}
            \State Increment $missing$ count and add issue to $quality\_issues$
            \State \textbf{continue}
        \EndIf
        \State $actual\_hash \leftarrow$ CalculateFileHash($expected\_path$)
        \If{$actual\_hash \neq file\_hash$}
            \State Increment $failed$ count and add hash mismatch issue
        \Else
            \State Increment $passed$ count
        \EndIf
    \EndFor
    \State Get $family\_statistics$
    \State Store $family\_statistics$ in $validation\_results$
    \State Analyze for $class\_imbalance$ and $minimum\_samples\_per\_family$
    \State Add $quality\_issues$ and $recommendations$ based on analysis
    \State Log validation completion
    \Return $validation\_results$
\EndProcedure
\Statex
\Function{GetFamilyStatistics}{}
    \State Query database for $family\_name, sample\_count, avg\_file\_size, first\_seen, last\_updated$ from `families` table
    \State Store results in a dictionary where keys are family names
    \Return Family statistics dictionary
\EndFunction
\Statex
\Procedure{CreateBalancedDatasetSplit}{$test\_size, validation\_size, min\_samples\_per\_family$}
    \State Query all $file\_hash, family\_label$ for 'collected' samples from database
    \State Group samples by family
    \State Filter families with $< min\_samples\_per\_family$
    \If{no valid families} \State Raise ValueError \EndIf
    \State Log balanced split creation details
    \State Initialize $train\_samples, val\_samples, test\_samples \leftarrow \emptyset$
    \For{each $family, family\_samples \in$ ValidFamilies}
        \State Split $family\_samples$ into train/validation/test sets using $train\_test\_split$ (stratified)
        \State Extend global $train\_samples, val\_samples, test\_samples$
        \State Log family split counts
    \EndFor
    \State Store split info ($train, validation, test$ sample hashes and metadata)
    \State Save split info to JSON file
    \State Log success
    \Return Split info dictionary
\EndProcedure
\Statex
\Function{GenerateDatasetReport}{}
    \State Initialize $report\_lines \leftarrow \emptyset$
    \State Add report header
    \State Query overall dataset statistics from database (total samples, total families, file size stats)
    \State Add Dataset Overview section
    \State Get $family\_statistics$ and add Family Distribution table
    \State Run ValidateDatasetIntegrity() and add Dataset Quality Assessment section (issues, recommendations)
    \State Save report to Markdown file
    \Return Generated report content as string
\EndFunction
\end{algorithmic}
\end{algorithm}
